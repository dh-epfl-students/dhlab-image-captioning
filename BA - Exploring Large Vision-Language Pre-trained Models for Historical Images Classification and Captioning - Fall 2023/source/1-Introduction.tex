% \section{Introduction} 
%\section{Background and Motivation} 

The \textit{impresso}\footnote{\url{https://impresso-project.ch/}} project -- ``\textit{Media Monitoring of the Past}'' carried out at the Digital Humanities Laboratory at EPFL is an interdisciplinary research project aiming for the datafication of a multilingual corpus of digitized historical newspapers. The project features a dataset of around 90 digitised historical newspapers containing approximately three million images. These images have no labels, and only 10\% of them have a caption, two aspects that hinder their retrieval. 

In the context of \textit{impresso}, we focus on exploring new ways of automatically classifying and captioning historical images, with the aim of integrating these techniques into historical research workflows.

%the enrichment of historical newspapers with automatically generated text 
%In this report, we focus on exploring new ways of automatically classifying and captioning historical images, without the need for fine-tuning the model on the dataset. (include motivation here or in the abstract)

% \section{Objectives \& Challenges}

Image classification and image captioning play crucial roles in the digital preservation and accessibility of historical newspapers by enabling efficient organization and categorization of historical newspaper content. This facilitates easy searching and retrieval of specific information, articles, or images, greatly enhancing the accessibility of these resources to researchers, historians, and the general public. However, there are several challenges in automatic image captioning and classification:

\begin{itemize}
    \item \textbf{Poor image quality}: Historical newspapers often suffer from fading, smudging, and physical damage, leading to poor image quality. This can hinder the accuracy of image recognition and captioning algorithms.
    \item \textbf{Complex layouts}: The varied and complex layouts, along with different font styles used in historical newspapers, can complicate text extraction and image classification.
    \item \textbf{Noise and artefacts}: Scanning artefacts, ink smudges, and paper quality issues introduce noise into the digitized images, making it difficult for algorithms to accurately identify and classify content.
    % \item \textbf{Contextual ambiguity}: Images in newspapers may lack clear context or be associated with multiple articles, making it challenging to generate accurate captions.
    \item  \textbf{Incomplete or missing captions}: Often, captions may be partially missing, illegible, or completely absent, necessitating the generation of entirely new captions based on the image content and surrounding context.
\end{itemize}

In this report, we explore several multimodal vision language models and offer insights on several research directions \parencite{bisk2020experience}:
\begin{enumerate}
    \item \textit{Are large vision models good at image classification?} -- We evaluate zero-shot image type classification of the available dataset using the CLIP (Contrastive Language–Image Pretraining) \parencite{radford2021learning}, and Flamingo \parencite{alayrac2022flamingo} in two settings, zero-shot and few-shot.
    We then compare with previous performances;
    \item \textit{What are the considerations for choosing a captioning model?} -- We evaluate image captioning on the same dataset, by an explorative study of prompting Flamingo, a multimodal, vision-language model, known for its ability in generating captions in two settings, zero-shot and few-shot. %This part will require adding caption information on the test part of the dataset. In addition to the fluency and accuracy of the generated captions, a specific aspect that might be taken into account is distinctiveness, i.e. whether the image contains details that differentiate it from similar images.
\end{enumerate}

% %Therefore, we explore in this report, first, an image classification task with a multimodal image-based model CLIP (Contrastive Language–Image Pretraining) %\parencite{radford2021learning} 
% that is known to excel at understanding and associating images with natural language descriptions. Second, we go towards image captioning by an explorative study of prompting a multimodal a visual language model, FLAMINGO %\parencite{alayrac2022flamingo}
% , that is known for its ability in generating captions,  in two settings, zero-shot and few-shot.



% (Draft) Evaluate if those kind of models can replace traditional ones, do they compete... To do this ...
% Evaluate zero-shot and few-shot image classification using a Vision-Language pre-trained models, namely CLIP \cite{[2]}, a Contrastive Language-image Pre-trained model and FLAMINGO, a familiy of VLMs \cite{[3]}.

% Compare the zero-shot and few-shot performances with the performance of a traditional VGG-16 model fine-tuned on the dataset.

% Evaluate captions generated by a visual language model based on its accuracy, its ability to recognize places ... [to be completed]



% \section{Challenges and Approach}
% Noisy data, not enough captions, digitizations time-consuming and expensive, ...
% For each challenge given in the previous section suggest an approach. Point to the corresponding chapters for each approach.




